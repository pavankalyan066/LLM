{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavankalyan066/LLM/blob/main/llama_2_13b_retrievalqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPdQvYmlWmNc"
      },
      "source": [
        "# RAG with LLaMa 13B\n",
        "\n",
        "In this notebook we'll explore how we can use the open source **Llama-13b-chat** model in both Hugging Face transformers and LangChain.\n",
        "At the time of writing, you must first request access to Llama 2 models via [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) (access is typically granted within a few hours).\n",
        "\n",
        "---\n",
        "\n",
        "üö® _Note that running this on CPU is sloooow. If running on Google Colab you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab._\n",
        "\n",
        "---\n",
        "\n",
        "We start by doing a `pip install` of all required libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "WuurvzoqSEo7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_fRq0BSGMBk"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU transformers==4.31.0 sentence-transformers==2.2.2 datasets==2.14.0 accelerate==0.21.0 einops==0.6.1 langchain==0.0.240 xformers==0.0.20 bitsandbytes==0.41.0\n",
        "# !pip install -qqq chromadb --progress-bar off\n",
        "!pip install -U langchain==0.0.332 chromadb==0.4.16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Hugging Face Embedding Pipeline\n",
        "\n",
        "We begin by initializing the embedding pipeline that will handle the transformation of our docs into vector embeddings. We will use the `sentence-transformers/all-MiniLM-L6-v2` model for embedding."
      ],
      "metadata": {
        "id": "fK7OXFdulxo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embed_model_id = 'thenlper/gte-large'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")"
      ],
      "metadata": {
        "id": "nQf0ICZXmGPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the embedding model to create document embeddings like so:"
      ],
      "metadata": {
        "id": "YSNke_aDnho-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"this is one document\",\n",
        "    \"and another document\"\n",
        "]\n",
        "\n",
        "embeddings = embed_model.embed_documents(docs)\n",
        "\n",
        "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
        "      f\"a dimensionality of {len(embeddings[0])}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4uRQacunhDP",
        "outputId": "caf4d863-ab10-4a5d-ea7c-b3eade81cbd0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 2 doc embeddings, each with a dimensionality of 1024.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Vector Index\n",
        "\n",
        "We now need to use the embedding pipeline to build our embeddings and store them in a Chromadb."
      ],
      "metadata": {
        "id": "E4SSLvJqqdhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import HuggingFaceDatasetLoader"
      ],
      "metadata": {
        "id": "lnGyY7NlW2gl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our index and embedding process ready we can move onto the indexing process itself. For that, we'll need a dataset. We will use a set of Arxiv papers related to (and including) the Llama 2 research paper."
      ],
      "metadata": {
        "id": "Dj2d7eJ3I8Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"jamescalam/llama-2-arxiv-papers-chunked\"\n",
        "page_content_column = \"chunk\"\n",
        "\n",
        "\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)"
      ],
      "metadata": {
        "id": "UKr21YMeXFAL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "LGHji_vBXE8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c8e4d6-69dd-4e73-bfc8-6526d740ebd3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2066: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=None' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyZ0SbTvXUvF",
        "outputId": "10db1d47-fbc7-4911-fac6-6ed0522bdee2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but', metadata={'doi': '1102.0183', 'chunk-id': '0', 'id': '1102.0183', 'title': 'High-Performance Neural Networks for Visual Object Classification', 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.', 'source': 'http://arxiv.org/pdf/1102.0183', 'authors': ['Dan C. Cire≈üan', 'Ueli Meier', 'Jonathan Masci', 'Luca M. Gambardella', 'J√ºrgen Schmidhuber'], 'categories': ['cs.AI', 'cs.NE'], 'comment': '12 pages, 2 figures, 5 tables', 'journal_ref': None, 'primary_category': 'cs.AI', 'published': '20110201', 'updated': '20110201', 'references': []})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data)):\n",
        "  data[i].metadata.pop('categories')\n",
        "  data[i].metadata.pop('authors')\n",
        "  data[i].metadata.pop('journal_ref')\n",
        "  data[i].metadata.pop('references')\n",
        "  data[i].metadata.pop('comment')"
      ],
      "metadata": {
        "id": "AhIcTFv6Wx5x"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[-1].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSds4IkRWHDg",
        "outputId": "a6f47455-0878-466b-e7a5-fda8b18d8a7a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doi': '2307.09288',\n",
              " 'chunk-id': '319',\n",
              " 'id': '2307.09288',\n",
              " 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models',\n",
              " 'summary': 'In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.',\n",
              " 'source': 'http://arxiv.org/pdf/2307.09288',\n",
              " 'primary_category': 'cs.CL',\n",
              " 'published': '20230718',\n",
              " 'updated': '20230719'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "ufT2s6qbMVMF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "vectordb = Chroma.from_documents(data, embedding=embeddings, persist_directory='./llama2_paper_')"
      ],
      "metadata": {
        "id": "92lo5rWkMVIj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQwEeW9Zps2"
      },
      "source": [
        "## Initializing the Hugging Face Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mElf068NXout"
      },
      "source": [
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `meta-llama/Llama-2-13b-chat-hf`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "d746af3aa96949a386e390189b86c0de",
            "85fb08c88c014fe8b5664cadb85a2bca",
            "78187f52c7354d30bfa834a0e5bb82ec",
            "e8579dad66664001851bb8fef67e8380",
            "0d7c686af50d443a8167915dd9ffef2c",
            "a83ca7d79fcd491b9964eb4c14eb7113",
            "09bc3c21fcaf4ba1aa4428d0fb11fbf1",
            "19c2200d97bf4942ba80b7ec14e5ed07",
            "1c59a164d63742479150d4305fc7d428",
            "c3ce72a02660499abc7efe32f78bddd2",
            "8bd6275303d645939b11e4e2aaf8d82f"
          ]
        },
        "id": "ikzdi_uMI7B-",
        "outputId": "83d09bab-5067-4da2-f8b5-c0567173d546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d746af3aa96949a386e390189b86c0de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzX9LqWSX9ot"
      },
      "source": [
        "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 13B models were trained using the Llama 2 13B tokenizer, which we initialize like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0iPv1GDGxgT",
        "outputId": "0f131d65-b01f-431f-c4c3-c1263df65719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id,use_auth_token=hf_auth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNysQFtPoaj7"
      },
      "source": [
        "Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qAYXi8ayKusU"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=200,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DG1WNTnJF1o"
      },
      "source": [
        "Confirm this is working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhFgmMr0JHUF",
        "outputId": "22f6aab1-6a20-4f9b-aabf-278a7eb3320b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain to me the difference between nuclear fission and fusion.\n",
            "\n",
            "Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing a large amount of energy in the process. This process typically occurs when an atom is bombarded with a high-energy particle, such as a neutron. When the nucleus splits, it releases a large amount of energy in the form of kinetic energy of the fragments and gamma radiation.\n",
            "\n",
            "Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases a large amount of energy, but it does so at much higher temperatures than those required for fission. In order to achieve fusion, the atoms must be heated to incredibly high temperatures, typically over 100 million degrees Celsius.\n",
            "\n",
            "One key difference between fission and fusion is the direction of the energy release. In\n"
          ]
        }
      ],
      "source": [
        "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
        "print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3W3cj3Re1K"
      },
      "source": [
        "Now to implement this in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-8RxQYwHRg0N"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "aiW0_FoQWG6J",
        "outputId": "ab4600c4-3cc3-44f0-c750-1aedb7bb6703"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nNuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing a large amount of energy in the process. This process typically occurs when an atom is bombarded with a high-energy particle, such as a neutron. When the nucleus splits, it releases a large amount of energy in the form of kinetic energy of the fragments and gamma radiation.\\n\\nNuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases a large amount of energy, but it does so at much higher temperatures than those required for fission. In order to achieve fusion, the atoms must be heated to incredibly high temperatures, typically over 100 million degrees Celsius.\\n\\nOne key difference between fission and fusion is the direction of the energy release. In'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tv0KxJLvsIa"
      },
      "source": [
        "We still get the same output as we're not really doing anything differently here, but we have now added **Llama 2 13B Chat** to the LangChain library. Using this we can now begin using LangChain's advanced agent tooling, chains, etc, with **Llama 2**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing a RetrievalQA Chain"
      ],
      "metadata": {
        "id": "hVu2KHaMLM2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **R**etrieval **A**ugmented **G**eneration (RAG) in LangChain we need to initialize either a `RetrievalQA` or `RetrievalQAWithSourcesChain` object. For both of these we need an `llm` (which we have initialized) and a vectordb ‚Äî but initialized within a LangChain vector store object.\n",
        "\n",
        "Let's begin by initializing the LangChain vector store, we do it like so:"
      ],
      "metadata": {
        "id": "0l9UNP7LLSXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can confirm this works like so:"
      ],
      "metadata": {
        "id": "-0dxBmDYpyj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what makes llama 2 special?'\n",
        "\n",
        "vectordb.similarity_search(\n",
        "    query,  # the search query\n",
        "    k=2  # returns top 3 most relevant chunks of text\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WhVonePp0hY",
        "outputId": "364d378a-965a-4484-8fd9-a283299393b6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Ricardo Lopez-Barquilla, Marc ShedroÔ¨Ä, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n‚Ä¢ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n‚Ä¢Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n‚Ä¢Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the Ô¨Ågures in the\\npaper.\\n‚Ä¢Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n‚Ä¢Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.', metadata={'chunk-id': '199', 'doi': '2307.09288', 'id': '2307.09288', 'primary_category': 'cs.CL', 'published': '20230718', 'source': 'http://arxiv.org/pdf/2307.09288', 'summary': 'In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'updated': '20230719'}),\n",
              " Document(page_content='our responsible release strategy can be found in Section 5.3.\\nTheremainderofthispaperdescribesourpretrainingmethodology(Section2),Ô¨Åne-tuningmethodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‚Ä°https://ai.meta.com/resources/models-and-libraries/llama/\\n¬ßWe are delaying the release of the 34B model due to a lack of time to suÔ¨Éciently red team.\\n¬∂https://ai.meta.com/llama\\n‚Äñhttps://github.com/facebookresearch/llama\\n4\\nFigure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication', metadata={'chunk-id': '14', 'doi': '2307.09288', 'id': '2307.09288', 'primary_category': 'cs.CL', 'published': '20230718', 'source': 'http://arxiv.org/pdf/2307.09288', 'summary': 'In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'updated': '20230719'})]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks good! Now we can put our `vectorstore` and `llm` together to create our RAG pipeline."
      ],
      "metadata": {
        "id": "r3zRCEcUqAGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "llyEC13RqF9B"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin asking questions! First let's try *without* RAG:"
      ],
      "metadata": {
        "id": "zjY7R4KDKZTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm('what is so special about llama 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "PnBrHM1PT7af",
        "outputId": "33d19998-8484-4547-e1c7-b593cac58fe1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAnswer: Llama 2 is a unique and special animal for several reasons. Here are some of the most notable features that make it stand out:\\n\\n1. Size: Llamas are known for their size, and Llama 2 is no exception. It is one of the largest llamas in existence, with some individuals reaching heights of over 6 feet (1.8 meters) at the shoulder and weighing up to 400 pounds (180 kilograms).\\n2. Coat: Llama 2 has a distinctive coat that is soft, fine, and silky to the touch. The coat can be a variety of colors, including white, cream, beige, and brown.\\n3. Temperament: Llama 2 is known for its friendly and docile nature. They are social animals that thrive on human interaction and are often used as therapy animals due to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, that's not what we meant... What if we use our RAG pipeline?"
      ],
      "metadata": {
        "id": "v33KdwE_Ua6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('what is so special about llama 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEndJT3_KYUi",
        "outputId": "9859c660-8b84-418d-c3bc-25cee86d293e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what is so special about llama 2?',\n",
              " 'result': ' Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. The models outperform open-source chat models on most benchmarks they tested, and based on their human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models.'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks *much* better! Let's try some more."
      ],
      "metadata": {
        "id": "1QK9mjspUhC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm('what safety measures were used in the development of llama 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "z65JYMpzUxE3",
        "outputId": "50d25af6-4f85-4879-a23f-3120e651ddad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nI'm looking for information on how the developers of Llama 2 ensured the safety of their users during the development process. Specifically, I'm interested in knowing about any safety measures that were implemented to protect users from potential risks or hazards associated with the use of the platform.\\n\\nHere are some possible answers:\\n\\n1. The developers of Llama 2 conducted thorough risk assessments to identify and mitigate any potential safety risks associated with the platform. This included identifying potential hazards such as data breaches, cyber attacks, and other security risks, and implementing appropriate safeguards to prevent these risks from occurring.\\n2. The platform was designed with user privacy and security in mind, and the developers implemented various measures to protect user data and ensure that it is not compromised. For example, the platform may have implemented encryption techniques to protect user data, or implemented strict access controls to limit who can access user information.\\n3. The developers of Llama 2 implemented a robust testing and quality assurance process to ensure that the platform is stable and reliable before it was released to users. This included conducting extensive testing to identify and fix any bugs or vulnerabilities that could potentially cause harm to users.\\n4. The platform was designed with scalability and reliability in mind, so that it can handle a large number of users and transactions without compromising performance or security. This includes implementing load balancing techniques, redundancy measures, and other infrastructure design best practices to ensure that the platform can handle high volumes of traffic and data without failing.\\n5. The developers of Llama 2 provided clear instructions and guidelines for users on how to safely use the platform, including tips on how to securely store and manage their personal information, and how to avoid common online scams and threats. Additionally, the platform may have provided resources such as customer support and educational materials to help users understand how to use the platform safely and effectively.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, it looks like the LLM with no RAG is less than ideal ‚Äî let's stop embarassing the poor LLM and stick with RAG + LLM. Let's ask the same question to our RAG pipeline."
      ],
      "metadata": {
        "id": "EPMfjpmhiJui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('what safety measures were used in the development of llama 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oR8DzztUli2",
        "outputId": "6581c6c2-d048-42fb-c8bf-6c3f9fa84e70"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what safety measures were used in the development of llama 2?',\n",
              " 'result': ' The authors of the paper mention that they attempted to reasonably balance safety with helpfulness in the development of LLAMA 2, but that their safety tuning may go too far in some instances, resulting in an overly cautious approach. They recommend that users of the pre-trained models should take extra steps in tuning and deployment as described in the Responsible Use Guide.'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reasonable answer from the RAG pipeline, but it doesn't contain much information ‚Äî maybe we can ask more about this, like what is this _\"red team\"_ procedure that delayed the launch of the 34B model?"
      ],
      "metadata": {
        "id": "Z2-I1A3MVZJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('what red teaming procedures were followed for llama 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GxZJVCBVS8Z",
        "outputId": "090507e0-75be-4ae8-e2aa-a3ee09a27ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what red teaming procedures were followed for llama 2?',\n",
              " 'result': \" The paper describes the red teaming procedures used for Llama 2. These included creating prompts that might elicit unsafe or undesirable responses from the model, such as those based on sensitive topics or those that could potentially cause harm if the model were to respond inappropriately. The red teaming exercises were performed by a set of experts who evaluated the model's responses and provided feedback on its performance. The paper also mentions that multiple additional rounds of red teaming were performed over several months to measure the robustness of the model as it was released internally.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very interesting!"
      ],
      "metadata": {
        "id": "vpMTOFzKivfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('how does the performance of llama 2 compare to other local LLMs?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xg4NQdYiv-P",
        "outputId": "9770d2de-3984-4361-b6a1-793594e95af5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'how does the performance of llama 2 compare to other local LLMs?',\n",
              " 'result': ' llama 2 has been shown to achieve state-of-the-art results on a variety of benchmarks, including XSum and Human Evaluation. It also has competitive performance compared to other local LLMs such as chinchilla and ars. However, the performance of llama 2 may vary depending on the specific task and hardware used.'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BfiGFDGGewNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d746af3aa96949a386e390189b86c0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85fb08c88c014fe8b5664cadb85a2bca",
              "IPY_MODEL_78187f52c7354d30bfa834a0e5bb82ec",
              "IPY_MODEL_e8579dad66664001851bb8fef67e8380"
            ],
            "layout": "IPY_MODEL_0d7c686af50d443a8167915dd9ffef2c"
          }
        },
        "85fb08c88c014fe8b5664cadb85a2bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a83ca7d79fcd491b9964eb4c14eb7113",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_09bc3c21fcaf4ba1aa4428d0fb11fbf1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "78187f52c7354d30bfa834a0e5bb82ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c2200d97bf4942ba80b7ec14e5ed07",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c59a164d63742479150d4305fc7d428",
            "value": 3
          }
        },
        "e8579dad66664001851bb8fef67e8380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ce72a02660499abc7efe32f78bddd2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8bd6275303d645939b11e4e2aaf8d82f",
            "value": " 3/3 [02:02&lt;00:00, 38.64s/it]"
          }
        },
        "0d7c686af50d443a8167915dd9ffef2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83ca7d79fcd491b9964eb4c14eb7113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09bc3c21fcaf4ba1aa4428d0fb11fbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19c2200d97bf4942ba80b7ec14e5ed07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c59a164d63742479150d4305fc7d428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3ce72a02660499abc7efe32f78bddd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd6275303d645939b11e4e2aaf8d82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}